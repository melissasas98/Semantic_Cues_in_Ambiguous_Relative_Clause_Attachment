{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Packages","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nimport torch\nimport re\nfrom transformers import BertTokenizer, BertForMaskedLM\nfrom transformers import RobertaTokenizer, RobertaForMaskedLM","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:35:38.295361Z","iopub.execute_input":"2023-08-17T09:35:38.296210Z","iopub.status.idle":"2023-08-17T09:35:38.301811Z","shell.execute_reply.started":"2023-08-17T09:35:38.296177Z","shell.execute_reply":"2023-08-17T09:35:38.300904Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Load Models","metadata":{}},{"cell_type":"code","source":"bert_base_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nbert_base_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n\nbert_large_tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\nbert_large_model = BertForMaskedLM.from_pretrained(\"bert-large-uncased\")\n\nroberta_base_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nroberta_base_model = RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n\nroberta_large_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\nroberta_large_model = RobertaForMaskedLM.from_pretrained(\"roberta-large\")","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:40:38.688437Z","iopub.execute_input":"2023-08-17T09:40:38.689012Z","iopub.status.idle":"2023-08-17T09:41:08.005322Z","shell.execute_reply.started":"2023-08-17T09:40:38.688978Z","shell.execute_reply":"2023-08-17T09:41:08.004209Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"Data/transformed_data.csv\", sep = \",\")\ndata[\"masked_token\"], data[\"predicted_token\"], data[\"accuracy\"] = \"\",\"\",\"\"\ndata[\"was_position\"], data[\"were_position\"] = '',''","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:05:32.417265Z","iopub.execute_input":"2023-08-17T10:05:32.418131Z","iopub.status.idle":"2023-08-17T10:05:32.451385Z","shell.execute_reply.started":"2023-08-17T10:05:32.418069Z","shell.execute_reply":"2023-08-17T10:05:32.450388Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#retrieve appropriate relative pronoun\nfor index, row in data.iterrows():\n    if row[\"number\"] == \"singular\":\n        data.loc[index, \"masked_token\"] = \"was\"\n    elif row[\"number\"] == \"plural\":\n        data.loc[index, \"masked_token\"] = \"were\"","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:05:33.864989Z","iopub.execute_input":"2023-08-17T10:05:33.865489Z","iopub.status.idle":"2023-08-17T10:05:34.039846Z","shell.execute_reply.started":"2023-08-17T10:05:33.865456Z","shell.execute_reply":"2023-08-17T10:05:34.038559Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T09:42:36.479032Z","iopub.execute_input":"2023-08-17T09:42:36.479523Z","iopub.status.idle":"2023-08-17T09:42:36.498260Z","shell.execute_reply.started":"2023-08-17T09:42:36.479488Z","shell.execute_reply":"2023-08-17T09:42:36.496767Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   id                                  original_sentence  \\\n0   0  A client looked at the clerk of the solicitor who   \n1   1  A client noticed the hairdresser of the actres...   \n2   2  A customer frowned at the assistant of the pha...   \n3   3    A fan looked at the guitarist of the singer who   \n4   4  A man asked for the apprentice of the builder who   \n\n                                transformed_sentence relative_pronoun  \\\n0  A client looked at the clerk of the offices wh...              who   \n1  A client noticed the hairdresser of the salons...              who   \n2  A customer frowned at the assistant of the lab...              who   \n3  A fan looked at the guitarist of the bands who...              who   \n4  A man asked for the apprentice of the colleges...              who   \n\n     number           N1          N2 exchanged_N2 masked_token  \\\n0  singular        clerk   solicitor       office          was   \n1  singular  hairdresser     actress        salon          was   \n2  singular    assistant  pharmacist   laboratory          was   \n3  singular    guitarist      singer         band          was   \n4  singular   apprentice     builder      college          was   \n\n  predicted_token accuracy was_position were_position  \n0                                                      \n1                                                      \n2                                                      \n3                                                      \n4                                                      ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>original_sentence</th>\n      <th>transformed_sentence</th>\n      <th>relative_pronoun</th>\n      <th>number</th>\n      <th>N1</th>\n      <th>N2</th>\n      <th>exchanged_N2</th>\n      <th>masked_token</th>\n      <th>predicted_token</th>\n      <th>accuracy</th>\n      <th>was_position</th>\n      <th>were_position</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>A client looked at the clerk of the solicitor who</td>\n      <td>A client looked at the clerk of the offices wh...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>clerk</td>\n      <td>solicitor</td>\n      <td>office</td>\n      <td>was</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>A client noticed the hairdresser of the actres...</td>\n      <td>A client noticed the hairdresser of the salons...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>hairdresser</td>\n      <td>actress</td>\n      <td>salon</td>\n      <td>was</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>A customer frowned at the assistant of the pha...</td>\n      <td>A customer frowned at the assistant of the lab...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>assistant</td>\n      <td>pharmacist</td>\n      <td>laboratory</td>\n      <td>was</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>A fan looked at the guitarist of the singer who</td>\n      <td>A fan looked at the guitarist of the bands who...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>guitarist</td>\n      <td>singer</td>\n      <td>band</td>\n      <td>was</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>A man asked for the apprentice of the builder who</td>\n      <td>A man asked for the apprentice of the colleges...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>apprentice</td>\n      <td>builder</td>\n      <td>college</td>\n      <td>was</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Mask the Auxiliary","metadata":{}},{"cell_type":"code","source":"#insert masked token in the position of the auxiliary\n#BERT and RoBERTa require different masked tokens\ndef mask_token(sent, model_type):\n    \n    tokens = nltk.word_tokenize(sent)\n    tag = nltk.pos_tag(tokens)\n    \n    masked_token = \" \"\n            \n    for i in range(len(tag)):\n        if tag[i][0] == \"was\" or tag[i][0] == \"were\":\n            if tag[i-1][0] == \"who\" or tag[i-1][0] == \"which\":\n                if model_type == \"BERT\":\n                    tokens[i] = \"[MASK]\"\n                elif model_type == \"RoBERTa\":\n                    tokens[i] = \"<mask>\"\n                \n    tokens = \" \".join(tokens)\n    \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:05:37.493492Z","iopub.execute_input":"2023-08-17T10:05:37.494511Z","iopub.status.idle":"2023-08-17T10:05:37.503594Z","shell.execute_reply.started":"2023-08-17T10:05:37.494472Z","shell.execute_reply":"2023-08-17T10:05:37.502228Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#create separate datasets for BERT and RoBERTa\nbert_data = data.copy()\nroberta_data = data.copy()\n\n#retrieve appropriate model type\nbert_data[\"model\"] = \"BERT\"\nroberta_data[\"model\"] = \"RoBERTa\"","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:05:39.915429Z","iopub.execute_input":"2023-08-17T10:05:39.915883Z","iopub.status.idle":"2023-08-17T10:05:39.924773Z","shell.execute_reply.started":"2023-08-17T10:05:39.915849Z","shell.execute_reply":"2023-08-17T10:05:39.923461Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#apply function to mask the auxiliary\nbert_data[\"transformed_sentence\"] = bert_data[\"transformed_sentence\"].apply(\n    mask_token, model_type = \"BERT\")\nroberta_data[\"transformed_sentence\"] = roberta_data[\"transformed_sentence\"].apply(\n    mask_token, model_type = \"RoBERTa\")","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:05:41.965322Z","iopub.execute_input":"2023-08-17T10:05:41.965739Z","iopub.status.idle":"2023-08-17T10:05:43.245091Z","shell.execute_reply.started":"2023-08-17T10:05:41.965709Z","shell.execute_reply":"2023-08-17T10:05:43.243654Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Get Prediction of the Models for the Masked Token","metadata":{}},{"cell_type":"code","source":"#source: https://gist.github.com/yuchenlin/a2f42d3c4378ed7b83de65c7a2222eb2\n#function to retrieve the top 500 predictions of the model for the masked token\ndef predict_masked_token(text, model_type, model, tokenizer, top_k=500):\n    \n    # Tokenize input depending on the model type\n    if model_type == \"BERT\":\n        text = \"[CLS] %s [SEP]\"%text\n        tokenized_text = tokenizer.tokenize(text)\n        masked_index = tokenized_text.index(\"[MASK]\")\n    elif model_type == \"RoBERTa\":\n        text = \"<s>%s</s>\"%text\n        tokenized_text = tokenizer.tokenize(text)\n        masked_index = tokenized_text.index(\"<mask>\")\n    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n    tokens_tensor = torch.tensor([indexed_tokens])\n\n    # Predict all tokens\n    with torch.no_grad():\n        outputs = model(tokens_tensor)\n        predictions = outputs[0]\n\n    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n    \n    predictions = []\n    \n    #retrieve top 500 predicted tokens \n    for i, pred_idx in enumerate(top_k_indices):\n        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n        predicted_token = re.sub(\"Ġ\", \"\", predicted_token) #delete Ġ from the predicted token\n        predictions.append(predicted_token)\n\n    position_was = \"\"\n    position_were = \"\"\n    \n    #retrieve the position of \"was\" and \"were\" in the predictions of the models\n    for i in range(len(predictions)):\n        if predictions[i] == \"was\":\n            position_was += str(predictions.index(\"was\")+1) #prevent position to be 0\n        elif predictions[i] == \"were\":\n            position_were += str(predictions.index(\"were\")+1) #prevent position to be 0\n    \n    highest_prediction = \"\"\n    \n    #check which of the auxiliaries the models predict first\n    for i in range(len(predictions)):  \n        if predictions[i] == \"was\" or predictions[i] == \"were\": #check whether predicted token is an auxiliary\n            highest_prediction += predictions[i]\n            break #only retrieve the first instance of the auxiliary\n            \n    return  pd.Series([highest_prediction, position_was, position_were])","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:18:06.832003Z","iopub.execute_input":"2023-08-17T10:18:06.832475Z","iopub.status.idle":"2023-08-17T10:18:06.846885Z","shell.execute_reply.started":"2023-08-17T10:18:06.832442Z","shell.execute_reply":"2023-08-17T10:18:06.845497Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"#create separate datasets for the model sizes\nbert_base = bert_data.copy()\nbert_large = bert_data.copy()\n\nroberta_base = roberta_data.copy()\nroberta_large = roberta_data.copy()\n\n#retrieve appropriate model size and the model type together with the type\nbert_base[\"size\"], bert_base[\"model_and_size\"] = \"base\", \"BERT-base\"\nbert_large[\"size\"], bert_large[\"model_and_size\"] = \"large\", \"BERT-large\"\nroberta_base[\"size\"], roberta_base[\"model_and_size\"] = \"base\", \"RoBERTa-base\"\nroberta_large[\"size\"], roberta_large[\"model_and_size\"] = \"large\", \"RoBERTa-large\"","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:18:10.178150Z","iopub.execute_input":"2023-08-17T10:18:10.178560Z","iopub.status.idle":"2023-08-17T10:18:10.193277Z","shell.execute_reply.started":"2023-08-17T10:18:10.178531Z","shell.execute_reply":"2023-08-17T10:18:10.191505Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#retrieve prediction for the masked token and the positon of the auxiliaries within the top 500 predictions\nbert_base[[\"predicted_token\", \"was_position\", \"were_position\"]] = bert_base[\"transformed_sentence\"].apply(\n    predict_masked_token, model_type = \"BERT\", model = bert_base_model, tokenizer = bert_base_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:18:11.883229Z","iopub.execute_input":"2023-08-17T10:18:11.883752Z","iopub.status.idle":"2023-08-17T10:19:17.930042Z","shell.execute_reply.started":"2023-08-17T10:18:11.883712Z","shell.execute_reply":"2023-08-17T10:19:17.929030Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"#retrieve prediction for the masked token and the positon of the auxiliaries within the top 500 predictions\nbert_large[[\"predicted_token\", \"was_position\", \"were_position\"]] = bert_large[\"transformed_sentence\"].apply(\n    predict_masked_token,model_type = \"BERT\", model = bert_large_model, tokenizer = bert_large_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:21:23.869333Z","iopub.execute_input":"2023-08-17T10:21:23.869753Z","iopub.status.idle":"2023-08-17T10:25:19.967971Z","shell.execute_reply.started":"2023-08-17T10:21:23.869721Z","shell.execute_reply":"2023-08-17T10:25:19.966434Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"#retrieve prediction for the masked token and the positon of the auxiliaries within the top 500 predictions\nroberta_base[[\"predicted_token\", \"was_position\", \"were_position\"]] = roberta_base[\"transformed_sentence\"].apply(\n    predict_masked_token, model_type = \"RoBERTa\",model = roberta_base_model, tokenizer = roberta_base_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:27:19.132733Z","iopub.execute_input":"2023-08-17T10:27:19.133221Z","iopub.status.idle":"2023-08-17T10:28:33.919370Z","shell.execute_reply.started":"2023-08-17T10:27:19.133186Z","shell.execute_reply":"2023-08-17T10:28:33.918136Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#retrieve prediction for the masked token and the positon of the auxiliaries within the top 500 predictions\nroberta_large[[\"predicted_token\", \"was_position\", \"were_position\"]] = roberta_large[\"transformed_sentence\"].apply(\n    predict_masked_token, model_type = \"RoBERTa\", model = roberta_large_model, tokenizer = roberta_large_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:35:41.563240Z","iopub.execute_input":"2023-08-17T10:35:41.563709Z","iopub.status.idle":"2023-08-17T10:39:46.002175Z","shell.execute_reply.started":"2023-08-17T10:35:41.563679Z","shell.execute_reply":"2023-08-17T10:39:46.000509Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the Performance of the Models","metadata":{}},{"cell_type":"code","source":"#function to check whether model predicted the appropriate auxiliary\n#assign 1 for correct predictions and 0 for incorrect ones\ndef evaluate_performance(predicted_token, masked_token):\n    if predicted_token == masked_token:\n        return 1\n    if predicted_token != masked_token:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:41:38.841723Z","iopub.execute_input":"2023-08-17T10:41:38.842528Z","iopub.status.idle":"2023-08-17T10:41:38.848645Z","shell.execute_reply.started":"2023-08-17T10:41:38.842490Z","shell.execute_reply":"2023-08-17T10:41:38.847473Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"#source: https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe\n#retrieve accuracy value of the models\nbert_base[\"accuracy\"] = bert_base[[\"predicted_token\",\"masked_token\"]].apply(\n    lambda x: evaluate_performance(*x), axis=1)\nbert_large[\"accuracy\"] = bert_large[[\"predicted_token\",\"masked_token\"]].apply(\n    lambda x: evaluate_performance(*x), axis=1)\nroberta_base[\"accuracy\"] = roberta_base[[\"predicted_token\",\"masked_token\"]].apply(\n    lambda x: evaluate_performance(*x), axis=1)\nroberta_large[\"accuracy\"] = roberta_large[[\"predicted_token\",\"masked_token\"]].apply(\n    lambda x: evaluate_performance(*x), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:44:09.154816Z","iopub.execute_input":"2023-08-17T10:44:09.155344Z","iopub.status.idle":"2023-08-17T10:44:09.214051Z","shell.execute_reply.started":"2023-08-17T10:44:09.155308Z","shell.execute_reply":"2023-08-17T10:44:09.212852Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"# Preparation of the Data for the Analysis","metadata":{}},{"cell_type":"code","source":"#combine datasets\nresults = pd.concat([bert_base, bert_large, roberta_base, roberta_large])","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:47:37.996632Z","iopub.execute_input":"2023-08-17T10:47:37.997116Z","iopub.status.idle":"2023-08-17T10:47:38.007897Z","shell.execute_reply.started":"2023-08-17T10:47:37.997058Z","shell.execute_reply":"2023-08-17T10:47:38.005813Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"#when model fails to make a prediction insert 0 to the position value\nresults[\"was_position\"] = results[\"was_position\"].replace(\"\", 0, regex=True)\nresults[\"was_position\"] = results[\"was_position\"].astype(int)\n\nresults[\"were_position\"] = results[\"were_position\"].replace(\"\", 0, regex=True)\nresults[\"were_position\"] = results[\"were_position\"].astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:47:39.663729Z","iopub.execute_input":"2023-08-17T10:47:39.664445Z","iopub.status.idle":"2023-08-17T10:47:39.677146Z","shell.execute_reply.started":"2023-08-17T10:47:39.664409Z","shell.execute_reply":"2023-08-17T10:47:39.675524Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"results[\"was_position_interval\"], results[\"were_position_interval\"] = \"\", \"\"","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:47:43.239728Z","iopub.execute_input":"2023-08-17T10:47:43.240422Z","iopub.status.idle":"2023-08-17T10:47:43.247827Z","shell.execute_reply.started":"2023-08-17T10:47:43.240384Z","shell.execute_reply":"2023-08-17T10:47:43.246033Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"#define position intervals\ndef assign_position_interval(position):\n    if position > 0 and position <= 5:\n        return 1\n    if position > 5 and position <= 10:\n        return 2\n    if position > 10 and position <= 15:\n        return 3\n    if position > 15 and position <= 20:\n        return 4\n    else:\n        return 5","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:49:44.696236Z","iopub.execute_input":"2023-08-17T10:49:44.696663Z","iopub.status.idle":"2023-08-17T10:49:44.703754Z","shell.execute_reply.started":"2023-08-17T10:49:44.696629Z","shell.execute_reply":"2023-08-17T10:49:44.702765Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"#assign position intervals to the dataset\nresults[\"was_position_interval\"] = results[\"was_position\"].apply(assign_position_interval)\nresults[\"were_position_interval\"] = results[\"were_position\"].apply(assign_position_interval)","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:49:46.521837Z","iopub.execute_input":"2023-08-17T10:49:46.522533Z","iopub.status.idle":"2023-08-17T10:49:46.536022Z","shell.execute_reply.started":"2023-08-17T10:49:46.522497Z","shell.execute_reply":"2023-08-17T10:49:46.534672Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"# Save the Data","metadata":{}},{"cell_type":"code","source":"#reorder columns\nresults = results[[\"id\", \"original_sentence\", \"transformed_sentence\", \"relative_pronoun\",\"number\",\n                   \"model\",\"size\", \"model_and_size\", \"N1\", \"N2\", \"exchanged_N2\", \"masked_token\", \n                   \"predicted_token\", \"accuracy\", \"was_position\", \"were_position\", \n                   \"was_position_interval\", \"were_position_interval\"]]","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:51:50.965427Z","iopub.execute_input":"2023-08-17T10:51:50.965960Z","iopub.status.idle":"2023-08-17T10:51:50.978694Z","shell.execute_reply.started":"2023-08-17T10:51:50.965925Z","shell.execute_reply":"2023-08-17T10:51:50.977137Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"results.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-17T10:55:30.259772Z","iopub.execute_input":"2023-08-17T10:55:30.260199Z","iopub.status.idle":"2023-08-17T10:55:30.280467Z","shell.execute_reply.started":"2023-08-17T10:55:30.260160Z","shell.execute_reply":"2023-08-17T10:55:30.279488Z"},"trusted":true},"execution_count":75,"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"   id                                  original_sentence  \\\n0   0  A client looked at the clerk of the solicitor who   \n1   1  A client noticed the hairdresser of the actres...   \n2   2  A customer frowned at the assistant of the pha...   \n3   3    A fan looked at the guitarist of the singer who   \n4   4  A man asked for the apprentice of the builder who   \n\n                                transformed_sentence relative_pronoun  \\\n0  A client looked at the clerk of the offices wh...              who   \n1  A client noticed the hairdresser of the salons...              who   \n2  A customer frowned at the assistant of the lab...              who   \n3  A fan looked at the guitarist of the bands who...              who   \n4  A man asked for the apprentice of the colleges...              who   \n\n     number model  size model_and_size           N1          N2 exchanged_N2  \\\n0  singular  BERT  base      BERT-base        clerk   solicitor       office   \n1  singular  BERT  base      BERT-base  hairdresser     actress        salon   \n2  singular  BERT  base      BERT-base    assistant  pharmacist   laboratory   \n3  singular  BERT  base      BERT-base    guitarist      singer         band   \n4  singular  BERT  base      BERT-base   apprentice     builder      college   \n\n  masked_token predicted_token  accuracy  was_position  were_position  \\\n0          was             was         1             8             34   \n1          was             was         1             6             22   \n2          was             was         1             6             19   \n3          was             was         1            11             13   \n4          was            were         0            16             10   \n\n   was_position_interval  were_position_interval  \n0                      2                       5  \n1                      2                       5  \n2                      2                       4  \n3                      3                       3  \n4                      4                       2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>original_sentence</th>\n      <th>transformed_sentence</th>\n      <th>relative_pronoun</th>\n      <th>number</th>\n      <th>model</th>\n      <th>size</th>\n      <th>model_and_size</th>\n      <th>N1</th>\n      <th>N2</th>\n      <th>exchanged_N2</th>\n      <th>masked_token</th>\n      <th>predicted_token</th>\n      <th>accuracy</th>\n      <th>was_position</th>\n      <th>were_position</th>\n      <th>was_position_interval</th>\n      <th>were_position_interval</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>A client looked at the clerk of the solicitor who</td>\n      <td>A client looked at the clerk of the offices wh...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>BERT</td>\n      <td>base</td>\n      <td>BERT-base</td>\n      <td>clerk</td>\n      <td>solicitor</td>\n      <td>office</td>\n      <td>was</td>\n      <td>was</td>\n      <td>1</td>\n      <td>8</td>\n      <td>34</td>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>A client noticed the hairdresser of the actres...</td>\n      <td>A client noticed the hairdresser of the salons...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>BERT</td>\n      <td>base</td>\n      <td>BERT-base</td>\n      <td>hairdresser</td>\n      <td>actress</td>\n      <td>salon</td>\n      <td>was</td>\n      <td>was</td>\n      <td>1</td>\n      <td>6</td>\n      <td>22</td>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>A customer frowned at the assistant of the pha...</td>\n      <td>A customer frowned at the assistant of the lab...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>BERT</td>\n      <td>base</td>\n      <td>BERT-base</td>\n      <td>assistant</td>\n      <td>pharmacist</td>\n      <td>laboratory</td>\n      <td>was</td>\n      <td>was</td>\n      <td>1</td>\n      <td>6</td>\n      <td>19</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>A fan looked at the guitarist of the singer who</td>\n      <td>A fan looked at the guitarist of the bands who...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>BERT</td>\n      <td>base</td>\n      <td>BERT-base</td>\n      <td>guitarist</td>\n      <td>singer</td>\n      <td>band</td>\n      <td>was</td>\n      <td>was</td>\n      <td>1</td>\n      <td>11</td>\n      <td>13</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>A man asked for the apprentice of the builder who</td>\n      <td>A man asked for the apprentice of the colleges...</td>\n      <td>who</td>\n      <td>singular</td>\n      <td>BERT</td>\n      <td>base</td>\n      <td>BERT-base</td>\n      <td>apprentice</td>\n      <td>builder</td>\n      <td>college</td>\n      <td>was</td>\n      <td>were</td>\n      <td>0</td>\n      <td>16</td>\n      <td>10</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#save dataset\noutput_file_path = \"Results/evaluated_data.csv\"\nresults.to_csv(output_file_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T14:53:39.058216Z","iopub.execute_input":"2023-07-29T14:53:39.058586Z","iopub.status.idle":"2023-07-29T14:53:39.109828Z","shell.execute_reply.started":"2023-07-29T14:53:39.058558Z","shell.execute_reply":"2023-07-29T14:53:39.108903Z"},"trusted":true},"execution_count":75,"outputs":[]}]}